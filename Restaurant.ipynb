{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.11/site-packages (0.21.0)\n",
      "Requirement already satisfied: Levenshtein==0.21.0 in /usr/local/lib/python3.11/site-packages (from python-Levenshtein) (0.21.0)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=2.3.0 in /usr/local/lib/python3.11/site-packages (from Levenshtein==0.21.0->python-Levenshtein) (3.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: py_stringmatching in /usr/local/lib/python3.11/site-packages (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.11/site-packages (from py_stringmatching) (1.24.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/site-packages (from py_stringmatching) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pyjarowinkler in /usr/local/lib/python3.11/site-packages (1.8)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: owlready2 in /usr/local/lib/python3.11/site-packages (0.41)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: strsimpy in /usr/local/lib/python3.11/site-packages (0.2.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/site-packages (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/site-packages (1.10.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/site-packages (3.7.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/site-packages (from scikit-learn) (1.24.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/site-packages (from matplotlib) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/alishalistya/Library/Python/3.11/lib/python/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/alishalistya/Library/Python/3.11/lib/python/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein\n",
    "!pip install py_stringmatching\n",
    "!pip install pyjarowinkler\n",
    "!pip install owlready2\n",
    "!pip install nltk\n",
    "!pip install strsimpy\n",
    "!pip install scikit-learn scipy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8050000071525574"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from py_stringmatching import utils\n",
    "from py_stringmatching.similarity_measure.jaro_winkler import JaroWinkler\n",
    "from py_stringmatching.similarity_measure.hybrid_similarity_measure import \\\n",
    "                                                    HybridSimilarityMeasure\n",
    "\n",
    "\n",
    "class MongeElkan(HybridSimilarityMeasure):\n",
    "    \"\"\"Computes Monge-Elkan measure.\n",
    "\n",
    "    The Monge-Elkan similarity measure is a type of hybrid similarity measure that combines the benefits of\n",
    "    sequence-based and set-based methods. This can be effective for domains in which more control is needed\n",
    "    over the similarity measure. It implicitly uses a secondary similarity measure, such as Levenshtein to compute\n",
    "    over all similarity score. See the string matching chapter in the DI book (Principles of Data Integration). \n",
    "\n",
    "    Args:\n",
    "        sim_func (function): Secondary similarity function. This is expected to be a sequence-based\n",
    "                             similarity measure (defaults to Jaro-Winkler similarity measure).\n",
    "\n",
    "    Attributes:\n",
    "        sim_func (function): An attribute to store the secondary similarity function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sim_func=JaroWinkler().get_raw_score):\n",
    "        self.sim_func = sim_func\n",
    "        super(MongeElkan, self).__init__()\n",
    "\n",
    "    def get_raw_score(self, bag1, bag2):\n",
    "            \"\"\"Computes the raw Monge-Elkan score between two bags (lists).\n",
    "\n",
    "            Args:\n",
    "                bag1,bag2 (list): Input lists.\n",
    "\n",
    "            Returns:\n",
    "                Monge-Elkan similarity score (float).\n",
    "\n",
    "            Raises:\n",
    "                TypeError : If the inputs are not lists or if one of the inputs is None.\n",
    "\n",
    "            Examples:\n",
    "                >>> me = MongeElkan()\n",
    "                >>> me.get_raw_score(['Niall'], ['Neal'])\n",
    "                0.8049999999999999\n",
    "                >>> me.get_raw_score(['Niall'], ['Nigel'])\n",
    "                0.7866666666666667\n",
    "                >>> me.get_raw_score(['Comput.', 'Sci.', 'and', 'Eng.', 'Dept.,', 'University', 'of', 'California,', 'San', 'Diego'], ['Department', 'of', 'Computer', 'Science,', 'Univ.', 'Calif.,', 'San', 'Diego'])\n",
    "                0.8677218614718616\n",
    "                >>> me.get_raw_score([''], ['a'])\n",
    "                0.0\n",
    "                >>> me = MongeElkan(sim_func=NeedlemanWunsch().get_raw_score)\n",
    "                >>> me.get_raw_score(['Comput.', 'Sci.', 'and', 'Eng.', 'Dept.,', 'University', 'of', 'California,', 'San', 'Diego'], ['Department', 'of', 'Computer', 'Science,', 'Univ.', 'Calif.,', 'San', 'Diego'])\n",
    "                2.0\n",
    "                >>> me = MongeElkan(sim_func=Affine().get_raw_score)\n",
    "                >>> me.get_raw_score(['Comput.', 'Sci.', 'and', 'Eng.', 'Dept.,', 'University', 'of', 'California,', 'San', 'Diego'], ['Department', 'of', 'Computer', 'Science,', 'Univ.', 'Calif.,', 'San', 'Diego'])\n",
    "                2.25\n",
    "\n",
    "            References:\n",
    "                * Principles of Data Integration book\n",
    "            \"\"\"\n",
    "            \n",
    "            # input validations\n",
    "            utils.sim_check_for_none(bag1, bag2)\n",
    "            utils.sim_check_for_list_or_set_inputs(bag1, bag2)\n",
    "\n",
    "            # if exact match return 1.0\n",
    "            if utils.sim_check_for_exact_match(bag1, bag2):\n",
    "                return 1.0\n",
    "\n",
    "            # if one of the strings is empty return 0\n",
    "            if utils.sim_check_for_empty(bag1, bag2):\n",
    "                return 0\n",
    "\n",
    "            # aggregated sum of all the max sim score of all the elements in bag1\n",
    "            # with elements in bag2\n",
    "            sum_of_maxes = 0\n",
    "            for el1 in bag1:\n",
    "                max_sim = float('-inf')\n",
    "                for el2 in bag2:\n",
    "                    max_sim = max(max_sim, self.sim_func(el1, el2))\n",
    "                sum_of_maxes += max_sim\n",
    "\n",
    "            sim = float(sum_of_maxes) / float(len(bag1))\n",
    "\n",
    "            return sim\n",
    "\n",
    "\n",
    "    def get_sim_func(self):\n",
    "            \"\"\"Get the secondary similarity function.\n",
    "\n",
    "            Returns:\n",
    "                secondary similarity function (function).\n",
    "            \"\"\"\n",
    "            return self.sim_func\n",
    "    def set_sim_func(self, sim_func):\n",
    "            \"\"\"Set the secondary similarity function.\n",
    "\n",
    "            Args:\n",
    "                sim_func (function): Secondary similarity function.\n",
    "            \"\"\"\n",
    "            self.sim_func = sim_func\n",
    "            return True\n",
    "me = MongeElkan()\n",
    "me.get_raw_score(['Niall'], ['Neal'])\n",
    "# me.get_raw_score(['Niall'], ['Nigel'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N gram Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-gram:  0.8 ['N', 'i', 'l', 'l']\n"
     ]
    }
   ],
   "source": [
    "def ngram(sentence, num):\n",
    "    tmp = [] \n",
    "    sent_len = len(sentence) - num +1\n",
    "    for i in range(sent_len):\n",
    "        tmp.append(sentence[i:i+num]) \n",
    "    return tmp\n",
    "\n",
    "def diff_ngram(sent_a, sent_b, num):\n",
    "    a = ngram(sent_a, num)\n",
    "    b = ngram(sent_b, num) \n",
    "    common = [] \n",
    "    cnt = 0 \n",
    "    for i in a:\n",
    "        for j in b:\n",
    "            if i == j:\n",
    "                cnt += 1\n",
    "                common.append(i)\n",
    "    return cnt/len(a), common\n",
    "\n",
    "c = \"Niall\"\n",
    "d = \"Nigel\"\n",
    "\n",
    "r2, word2 = diff_ngram(c, d ,1)\n",
    "print(\"2-gram: \", r2, word2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Simililarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "\n",
    "def text_to_vector(text):\n",
    "    word = re.compile(r'\\w+')\n",
    "    words = word.findall(text)\n",
    "    return Counter(words)\n",
    "\n",
    "\n",
    "def get_result(content_a, content_b):\n",
    "    text1 = content_a\n",
    "    text2 = content_b\n",
    "\n",
    "    vector1 = text_to_vector(text1)\n",
    "    vector2 = text_to_vector(text2)\n",
    "\n",
    "    cosine_result = get_cosine(vector1, vector2)\n",
    "    return cosine_result\n",
    "\n",
    "print(get_result('restaurant', 'restaurant')) #0.65565\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Levenshtein Similairity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Levenshtein\n",
    "Levenshtein.distance(\"Niall\", \"Nigel\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aspect in Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FOOD#PRICES', 'FOOD#QUALITY', 'FOOD#STYLE_OPTIONS']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['AMBIENCE#GENERAL']\n",
    "['DRINKS#PRICES', 'DRINKS#QUALITY', 'DRINKS#STYLE_OPTIONS']\n",
    "['RESTAURANT#MISCELLANEOUS']\n",
    "['LOCATION#GENERAL']\n",
    "['DRINKS#PRICES', 'FOOD#PRICES', 'RESTAURANT#PRICES']\n",
    "['RESTAURANT#GENERAL']\n",
    "['SERVICE#GENERAL']\n",
    "['DRINKS#STYLE_OPTIONS', 'FOOD#STYLE_OPTIONS']\n",
    "['FOOD#PRICES', 'FOOD#QUALITY', 'FOOD#STYLE_OPTIONS']\n",
    "\n",
    "#Catatan\n",
    "#sustenance enggak tau masuk aspek yang mana"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadXml(dom,topic):\n",
    "    xml_text = []\n",
    "    xml_target = []\n",
    "    xml_aspect = []\n",
    "    xml_sentiment = []\n",
    "    for node in topic:\n",
    "        opini=node.getElementsByTagName('Opinion')\n",
    "        temp_target = []\n",
    "        temp_aspect = []\n",
    "        temp_sentiment = []\n",
    "        for o in opini:\n",
    "            temp_target.append(o.attributes['target'].value)\n",
    "            temp_aspect.append(o.attributes['category'].value)\n",
    "            temp_sentiment.append(o.attributes['polarity'].value)\n",
    "        xml_text.append(node.getElementsByTagName('text')[0].firstChild.data)\n",
    "        xml_target.append(temp_target)\n",
    "        xml_aspect.append(temp_aspect)\n",
    "        xml_sentiment.append(temp_sentiment)\n",
    "    return xml_text, xml_target, xml_aspect, xml_sentiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ive asked a cart, attendant for and lotus leaf wrapped rice, and she replied back rice and just; walked; away\n",
      "['Ive asked a cart', 'attendant for', 'lotus leaf wrapped rice', 'and she replied back rice', 'just', 'walked', 'away']\n"
     ]
    }
   ],
   "source": [
    "def split_delimiter(text):\n",
    "    text_split = text.split(\", and \")\n",
    "    text_split_arr = []\n",
    "    text_split_result_arr = []\n",
    "    for text1 in text_split:\n",
    "    #     print(text)\n",
    "        text_split = text1.split(\" and \")\n",
    "        for text_split_result in text_split:\n",
    "            text_split_arr.append(text_split_result)\n",
    "    for text1 in text_split_arr:\n",
    "        text_split = text1.split(\", \")\n",
    "        for text_split_result in text_split:\n",
    "            text_split_result_arr.append(text_split_result)\n",
    "    return text_split_result_arr\n",
    "\n",
    "import re\n",
    "def split_delimiter2(text):\n",
    "    return re.split('; |, |, and | and ', text)\n",
    "\n",
    "text = \"Ive asked a cart, attendant for and lotus leaf wrapped rice, and she replied back rice and just; walked; away\"\n",
    "text_split = split_delimiter2(text)\n",
    "print(text)\n",
    "print(text_split)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jml_data_train_persentase(xml_text,persentase_train):\n",
    "    jml_train = round(len(xml_text)*persentase_train)\n",
    "    return jml_train\n",
    "\n",
    "def split_to_test_and_train(xml_text, xml_target, xml_aspect, xml_sentiment,jml_train):\n",
    "    xml_text_train = xml_text[0:jml_train]\n",
    "    xml_text_test = xml_text[jml_train:len(xml_text)]\n",
    "    xml_target_train = xml_target[0:jml_train]\n",
    "    xml_target_test = xml_target[jml_train:len(xml_text)]\n",
    "    xml_aspect_train = xml_aspect[0:jml_train]\n",
    "    xml_aspect_test = xml_aspect[jml_train:len(xml_text)]\n",
    "    xml_sentiment_train = xml_sentiment[0:jml_train]\n",
    "    xml_sentiment_test = xml_sentiment[jml_train:len(xml_text)]\n",
    "    return xml_text_train, xml_text_test, xml_target_train, xml_target_test, xml_aspect_train, xml_aspect_test, xml_sentiment_train, xml_sentiment_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentence):\n",
    "    # gunakan fungsi .lower() untuk case folding\n",
    "    lowercase_sentence = sentence.lower()\n",
    "\n",
    "    #remove angka\n",
    "    lowercase_sentence = re.sub(r\"\\d+\", \"\", lowercase_sentence)\n",
    "\n",
    "    #remove whitespace leading & trailing\n",
    "    lowercase_sentence = lowercase_sentence.strip()\n",
    "\n",
    "    #remove multiple whitespace into single whitespace\n",
    "    lowercase_sentence = re.sub('\\s+',' ',lowercase_sentence)\n",
    "\n",
    "    tokens = nltk.tokenize.word_tokenize(lowercase_sentence)\n",
    "#     print(tokens)\n",
    "# #     #Lemmatization\n",
    "#     lemma = []\n",
    "#     wordnet_lemmatizer = WordNetLemmatizer()\n",
    "#     for w in tokens:\n",
    "#     #     print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))\n",
    "#         lemma.append(wordnet_lemmatizer.lemmatize(w))\n",
    "        \n",
    "    # Stemming    \n",
    "#     from nltk.stem.porter import PorterStemmer\n",
    "#     porter_stemmer  = PorterStemmer()\n",
    "#     for w in tokens:\n",
    "# #         print(\"Stemming for {} is {}\".format(w,porter_stemmer.stem(w)))\n",
    "#         lemma.append(porter_stemmer.stem(w))\n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print multi aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data(xml_text, xml_target, xml_aspect, xml_sentiment,jml_print):\n",
    "    row = \"{name1:^20}|{name2:^20}|{name3:^20}\".format\n",
    "#     for index_text in range(len(xml_text)):\n",
    "    for index_text in range(jml_print):\n",
    "        print(\"Text: \"+str(xml_text[index_text]))\n",
    "        for i in range(len(xml_target[index_text])):\n",
    "            print(\"- Target: \"+str(xml_target[index_text][i])+\" | Aspect: \"+str(xml_aspect[index_text][i])+\" | Sentiment: \"+str(xml_sentiment[index_text][i]))\n",
    "        print(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print One aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_split_text(xml_text, xml_target, xml_aspect, xml_sentiment,jml_print):\n",
    "    row = \"{name1:^20}|{name2:^20}|{name3:^20}\".format\n",
    "#     for index_text in range(len(xml_text)):\n",
    "    for index_text in range(jml_print):\n",
    "        print(\"Text: \"+str(xml_text[index_text]))\n",
    "        print(\"- Target: \"+str(xml_target[index_text])+\" | Aspect: \"+str(xml_aspect[index_text])+\" | Sentiment: \"+str(xml_sentiment[index_text]))\n",
    "        print(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete multi index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def deleted_multi_index(arr,indexes):\n",
    "    for index in sorted(indexes, reverse=True):\n",
    "        del arr[index]\n",
    "    return arr\n",
    "\n",
    "arr = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "indexes = [2, 3, 5]\n",
    "deleted_multi_index(arr,indexes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data awal\n",
      "Text: Service was devine, oysters where a sensual as they come, and the price can't be beat!!!\n",
      "- Target: restaurant | Aspect: RESTAURANT#GENERAL | Sentiment: positif\n",
      "- Target: restaurant | Aspect: FOOD#QUALITY | Sentiment: positif\n",
      "\n",
      "Hasil\n",
      "Text: Service was devine, oysters where a sensual as they come, and the price can't be beat!!!\n",
      "- Target: restaurant | Aspect: RESTAURANT#GENERAL | Sentiment: positif\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def search_index_multi(arr,value):\n",
    "    return [i for i, x in enumerate(arr) if x == value]\n",
    "\n",
    "def clear_double_target(xml_text, xml_target, xml_aspect, xml_sentiment):\n",
    "    for index_text in range(len(xml_text)):\n",
    "        if (len(xml_target[index_text])>1):\n",
    "#             print(\"1\"+str(xml_target[index_text]))\n",
    "            jml_xml_target = len(xml_target[index_text]) - 1\n",
    "#             for i in range(jml_xml_target):\n",
    "            i = 0    \n",
    "            while i < jml_xml_target :\n",
    "#                 print(search_index_multi(xml_target[index_text],xml_target[index_text][i]))\n",
    "                arr_pop = search_index_multi(xml_target[index_text],xml_target[index_text][i])\n",
    "                i +=1\n",
    "                index_pop = len(arr_pop) -1\n",
    "                while index_pop>0:\n",
    "#                     print(xml_target[index_text][arr_pop[index_pop]])\n",
    "#                     print(\"-- \"+str(arr_pop[index_pop]))\n",
    "                    xml_target[index_text].pop(arr_pop[index_pop])\n",
    "                    xml_aspect[index_text].pop(arr_pop[index_pop])\n",
    "                    xml_sentiment[index_text].pop(arr_pop[index_pop])\n",
    "                    index_pop -= 1\n",
    "            \n",
    "                jml_xml_target = len(xml_target[index_text]) - 1\n",
    "#                 xml_target[index_text].pop(0)\n",
    "#                 jml_xml_target = jml_xml_target - 1\n",
    "#             print(\"2\"+str(xml_target[index_text]))\n",
    "    return xml_text, xml_target, xml_aspect, xml_sentiment\n",
    "\n",
    "x_text = [\"Service was devine, oysters where a sensual as they come, and the price can't be beat!!!\"]\n",
    "x_target = [[\"restaurant\",\"restaurant\"]]\n",
    "x_aspect = [[\"RESTAURANT#GENERAL\",\"FOOD#QUALITY\"]]\n",
    "x_sentiment = [[\"positif\",\"positif\"]]\n",
    "print(\"Data awal\")\n",
    "print_data(x_text, x_target, x_aspect, x_sentiment,1)\n",
    "\n",
    "x_text, x_target, x_aspect, x_sentiment = clear_double_target(x_text, x_target, x_aspect, x_sentiment)\n",
    "print(\"Hasil\")\n",
    "print_data(x_text, x_target, x_aspect, x_sentiment,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search Index Value yg sama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[5, 6, 11, 13]\n"
     ]
    }
   ],
   "source": [
    "add = [1,2,3,4,5,6,6,7,8,9,7,6,4,6]\n",
    "print(add.index(6))\n",
    "\n",
    "def index_multi(arr,value):\n",
    "    return [i for i, x in enumerate(arr) if x == value]\n",
    "\n",
    "a = index_multi(add,6)\n",
    "print(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing multi aspect to one aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Satu text dengan multi aspect =====\n",
      "Text: Judging from previous posts this used to be a good place, but not any longer.\n",
      "- Target: place | Aspect: RESTAURANT#GENERAL | Sentiment: negative\n",
      "\n",
      "Text: We, there were four of us, arrived at noon - the place was empty - and the staff acted like we were imposing on them and they were very rude.\n",
      "- Target: staff | Aspect: SERVICE#GENERAL | Sentiment: negative\n",
      "\n",
      "Text: They never brought us complimentary noodles, ignored repeated requests for sugar, and threw our dishes on the table.\n",
      "- Target: NULL | Aspect: SERVICE#GENERAL | Sentiment: negative\n",
      "\n",
      "Text: The food was lousy - too sweet or too salty and the portions tiny.\n",
      "- Target: food | Aspect: FOOD#QUALITY | Sentiment: negative\n",
      "- Target: portions | Aspect: FOOD#STYLE_OPTIONS | Sentiment: negative\n",
      "\n",
      "Text: After all that, they complained to me about the small tip.\n",
      "- Target: NULL | Aspect: SERVICE#GENERAL | Sentiment: negative\n",
      "\n",
      "----> Menjadi <-----\n",
      "===== Satu text dengan satu aspect =====\n",
      "Text: Judging from previous posts this used to be a good place, but not any longer.\n",
      "- Target: place | Aspect: RESTAURANT#GENERAL | Sentiment: negative\n",
      "\n",
      "Text: We, there were four of us, arrived at noon - the place was empty - and the staff acted like we were imposing on them and they were very rude.\n",
      "- Target: staff | Aspect: SERVICE#GENERAL | Sentiment: negative\n",
      "\n",
      "Text: They never brought us complimentary noodles, ignored repeated requests for sugar, and threw our dishes on the table.\n",
      "- Target: NULL | Aspect: SERVICE#GENERAL | Sentiment: negative\n",
      "\n",
      "Text: The food was lousy - too sweet or too salty\n",
      "- Target: food | Aspect: FOOD#QUALITY | Sentiment: negative\n",
      "\n",
      "Text: the portions tiny.\n",
      "- Target: portions | Aspect: FOOD#STYLE_OPTIONS | Sentiment: negative\n",
      "\n",
      "2153\n"
     ]
    }
   ],
   "source": [
    "import xml.dom.minidom\n",
    "from xml.dom.minidom import Node\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import \tWordNetLemmatizer\n",
    "from owlready2 import *\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "nltk.download('all')\n",
    "# def chunking22(text):\n",
    "#     result = []\n",
    "#     result_chunking = []\n",
    "#     tokens_tag = pos_tag(text)\n",
    "#     patterns= \"NP:{<DT|PP|CD|RB>?<JJ|JJR|JJS>*<NN|NNS|PRP|NNP|VB|IN|PRP\\$>+<VBD|VBZ|VBN|VBP|VB|IN>*<JJ|JJS|RB>*<PRP|NN|NNS>*}\"\n",
    "#     chunker = RegexpParser(patterns)\n",
    "# #     print(\"After Regex:\",chunker)\n",
    "#     output = chunker.parse(tokens_tag)\n",
    "#     print(\"After Chunking\",output)\n",
    "# #     print(\"\")\n",
    "# #     print(\"After Chunking Parse\")\n",
    "# #     for i in output:\n",
    "# # #         print(i)\n",
    "# # #         print(len(i))\n",
    "# #         result_chunking = []\n",
    "# #         for j in i:\n",
    "# # #             print(j[0])\n",
    "# #             result_chunking.append(j[0])\n",
    "# #         result.append(result_chunking)\n",
    "# #     return result  \n",
    "\n",
    "def ontology_ABSA_mention(sentence_1):\n",
    "    result_mention = []\n",
    "    result_sentiment = []\n",
    "    result_mention_aspect = []\n",
    "    result_mention_subclass = []\n",
    "#     print (\"{:<14} {:<13} {:<15} {:<10} {:<15}\".format(\"Term\", \"Sentiment\",\"Aspect\", \"Sub_Class\", \"lexicon\"))\n",
    "#     print(\"----------------------------------------------------------------\")\n",
    "#     print(\"total mention\"+str(mention))\n",
    "#     print(\"total sentence\"+str(sentence_1))\n",
    "    for x in range(len(sentence_1)):\n",
    "        for i in range(len(mention)):\n",
    "            if sentence_1[x] == mention[i]:\n",
    "                result_mention.append(mention[i])\n",
    "                result_sentiment.append(mention_sentiment[i])\n",
    "                result_mention_aspect.append(mention_aspect[i])\n",
    "                result_mention_subclass.append(mention_subclass[i])\n",
    "                \n",
    "#     for i in range(len(mention)):\n",
    "# #         print(\"mention  \"+mention[i])\n",
    "#         if mention[i] in sentence_1:\n",
    "# #             print (\"{:<14} {:<13} {:<15} {:<10} {:<15}\".format(str(mention[i]), str(mention_sentiment[i]),str(mention_aspect[i]), str(mention_subclass[i]),str(mention_lex[i])))\n",
    "#             result_mention.append(mention[i])\n",
    "#             result_sentiment.append(mention_sentiment[i])\n",
    "#             result_mention_aspect.append(mention_aspect[i])\n",
    "#             result_mention_subclass.append(mention_subclass[i])\n",
    "    \n",
    "    return result_mention,result_sentiment,result_mention_aspect,result_mention_subclass\n",
    "\n",
    "def ontology_ABSA_sentiment(sentence_1):\n",
    "    result_sentiment_term = []\n",
    "    result_sentiment = []\n",
    "    result_sentiment_aspect = []\n",
    "#     print (\"{:<14} {:<13} {:<15} \".format(\"Term\", \"Sentiment\",\"Aspect\"))\n",
    "#     print(\"----------------------------------------------------------------\")\n",
    "    for i in range(len(sentiment_term)-1):\n",
    "        if sentiment_term[i] in sentence_1:\n",
    "#             print(\"{:<14} {:<13} {:<15}\".format(str(sentiment_term[i]),str(sentiment[i]),str(sentiment_aspect[i])))\n",
    "            result_sentiment_term.append(sentiment_term[i])\n",
    "            result_sentiment.append(sentiment[i])\n",
    "            result_sentiment_aspect.append(sentiment_aspect[i])\n",
    "            \n",
    "    return result_sentiment_term,result_sentiment,result_sentiment_aspect\n",
    "\n",
    "## Get Term from Ontology\n",
    "# onto = get_ontology(\"C:/Users/akhmad.saifullah/opencv/Scripts/Tesis/ontology.owl\").load()\n",
    "\n",
    "# ## Get Data\n",
    "# dom = xml.dom.minidom.parse(\"D:/Paper S2/Data/Semval-2016/ABSA16_Restaurants_Train_SB1_v2 (2000).xml\")\n",
    "# # dom = xml.dom.minidom.parse(\"D:/Paper S2/[Matakuliah] Implementasi Proyek 2/SemEval2016 6000 data/EN_REST_SB2_TEST.xml.gold\")\n",
    "# text=dom.getElementsByTagName('text')\n",
    "# print(len(text))\n",
    "\n",
    "dom = xml.dom.minidom.parse(\"ABSA16_Restaurants_Train_SB1_v2 (2000).xml\")\n",
    "# dom = xml.dom.minidom.parse(\"D:/Paper S2/Data/Semval-2016/EN_REST_SB1_TEST.xml.gold\")\n",
    "topic=dom.getElementsByTagName('sentence')\n",
    "\n",
    "xml_text, xml_target, xml_aspect, xml_sentiment = ReadXml(dom,topic)\n",
    "\n",
    "xml_text, xml_target, xml_aspect, xml_sentiment = clear_double_target(xml_text, xml_target, xml_aspect, xml_sentiment)\n",
    "\n",
    "# jml_data_train = jml_data_train_persentase(xml_text,0.1)\n",
    "# xml_text_train, xml_text_test, xml_target_train, xml_target_test, xml_aspect_train, xml_aspect_test, xml_sentiment_train, xml_sentiment_test = split_to_test_and_train(xml_text, xml_target, xml_aspect, xml_sentiment,jml_data_train)\n",
    "# print((xml_text_train))\n",
    "# print(\"\")\n",
    "# print((xml_text_test))\n",
    "print(\"===== Satu text dengan multi aspect =====\")\n",
    "print_data(xml_text, xml_target, xml_aspect, xml_sentiment,5)\n",
    "# proses Preprocessing\n",
    "\n",
    "arr = [] #hasil Spliting text sehingga tidak ada lagi satu text beberapa sentiment dan aspect\n",
    "def xml_text_per_target(xml_text, xml_target, xml_aspect, xml_sentiment): \n",
    "    new_xml_text = []\n",
    "    new_xml_target = []\n",
    "    new_xml_aspect = []\n",
    "    new_xml_sentiment = []\n",
    "    for index_text in range(len(xml_text)):\n",
    "    # for index_text in range(500): # Proses Split data dan preprocessing\n",
    "    #     print(str(index_text)+\". \"+str(xml_text[index_text]))\n",
    "        status_null= 0\n",
    "        split_text_deleted_index = []\n",
    "        if(len(xml_target[index_text])>1): # deteksi target per split text (not null target) \n",
    "            split_text = split_delimiter2(xml_text[index_text]) #Spliting data dengan delimiter\n",
    "            i = 0\n",
    "            while i < len(xml_target[index_text]):\n",
    "                current_target = xml_target[index_text][i]\n",
    "                if current_target == 'NULL':\n",
    "                    status_null = 1\n",
    "                j = 0\n",
    "                while j < len(split_text):\n",
    "                    if current_target in split_text[j] and \" \" in split_text[j]:\n",
    "                        split_text_deleted_index.append(j)\n",
    "                        new_xml_text.append(split_text[j])\n",
    "                        new_xml_target.append(xml_target[index_text][i])\n",
    "                        new_xml_aspect.append(xml_aspect[index_text][i])\n",
    "                        new_xml_sentiment.append(xml_sentiment[index_text][i])\n",
    "                        split_text[j] = \"\"\n",
    "                    j+= 1\n",
    "                i +=1\n",
    "        if(status_null==1):\n",
    "            index_null_target = xml_target[index_text].index('NULL')\n",
    "            jj=0\n",
    "            temp_len_split_text= -1\n",
    "            index_split_text = -1\n",
    "            while jj < len(split_text):\n",
    "#            \n",
    "                current_len_split_text = len(split_text[jj])\n",
    "                if temp_len_split_text < current_len_split_text and current_len_split_text!='':\n",
    "                    temp_len_split_text = current_len_split_text\n",
    "                    index_split_text = jj\n",
    "                jj+=1\n",
    "            if index_split_text != -1 and \" \" in split_text[index_split_text]:\n",
    "                new_xml_text.append(split_text[index_split_text])\n",
    "                new_xml_target.append(xml_target[index_text][index_null_target])\n",
    "                new_xml_aspect.append(xml_aspect[index_text][index_null_target])\n",
    "                new_xml_sentiment.append(xml_sentiment[index_text][index_null_target])\n",
    "        elif (str(xml_target[index_text]) !='[]' and len(xml_target[index_text])==1) :\n",
    "            new_xml_text.append(xml_text[index_text])\n",
    "            new_xml_target.append(xml_target[index_text][0])\n",
    "            new_xml_aspect.append(xml_aspect[index_text][0])\n",
    "            new_xml_sentiment.append(xml_sentiment[index_text][0])\n",
    "#             print(xml_target[index_text][0])\n",
    "    return new_xml_text, new_xml_target, new_xml_aspect, new_xml_sentiment\n",
    "# Result After Preprocess\n",
    "xml_text, xml_target,xml_aspect,xml_sentiment = xml_text_per_target(xml_text, xml_target, xml_aspect, xml_sentiment)\n",
    "print(\"----> Menjadi <-----\")\n",
    "print(\"===== Satu text dengan satu aspect =====\")\n",
    "print_data_split_text(xml_text, xml_target,xml_aspect,xml_sentiment,5)\n",
    "print(len(xml_text))\n",
    "# print(arr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2153\n",
      "2058\n"
     ]
    }
   ],
   "source": [
    "# print_data_split_text(xml_text, xml_target,xml_aspect,xml_sentiment,5)\n",
    "print(len(xml_text))\n",
    "def delete_neutral(xml_text, xml_target,xml_aspect,xml_sentiment):\n",
    "  new_xml_text = []\n",
    "  new_xml_target = []\n",
    "  new_xml_aspect = []\n",
    "  new_xml_sentiment = []\n",
    "  for jj in range(len(xml_text)):\n",
    "    if xml_sentiment[jj]!=\"neutral\":\n",
    "      new_xml_text.append(xml_text[jj])\n",
    "      new_xml_target.append(xml_target[jj])\n",
    "      new_xml_aspect.append(xml_aspect[jj])\n",
    "      new_xml_sentiment.append(xml_sentiment[jj])\n",
    "  return new_xml_text, new_xml_target, new_xml_aspect, new_xml_sentiment\n",
    "xml_text, xml_target,xml_aspect,xml_sentiment= delete_neutral(xml_text, xml_target,xml_aspect,xml_sentiment)\n",
    "print(len(xml_text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read OWL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term           Sentiment     Aspect          Sub_Class \n",
      "---------------------------------------------------------------\n",
      "expensive      negative      ['DRINKS#PRICES', 'FOOD#PRICES'] PriceMention ['expensive']  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from owlready2 import *\n",
    "onto = get_ontology(\"ontology.owl\").load()\n",
    "list(onto.properties())\n",
    "\n",
    "mention = []\n",
    "mention_aspect = []\n",
    "mention_sentiment = []\n",
    "mention_subclass = []\n",
    "mention_axiom = []\n",
    "mention_lex = []\n",
    "\n",
    "def save_mention(name_mention,name_aspect,name_sentiment,name_subclass,name_lex):\n",
    "    \n",
    "    if len(name_lex)==1 and name_mention not in mention:\n",
    "        mention.append(name_mention)\n",
    "        mention_aspect.append(name_aspect)\n",
    "        mention_sentiment.append(name_sentiment)\n",
    "        mention_subclass.append(name_subclass)\n",
    "        mention_lex.append(name_lex)\n",
    "    else:\n",
    "        for i in name_lex:\n",
    "            if i not in mention:\n",
    "                mention.append(i)\n",
    "                mention_aspect.append(name_aspect)\n",
    "                mention_sentiment.append(name_sentiment)\n",
    "                mention_subclass.append(name_subclass)\n",
    "                mention_lex.append(name_lex)\n",
    "    \n",
    "for i in onto.Mention.subclasses():\n",
    "    for j in i.subclasses():\n",
    "        sentiment_current = ''\n",
    "        aspect_current = []\n",
    "        aspect_current = j.aspect\n",
    "        subclass_current = j.name\n",
    "        if str(j).find('Positive')!= -1:\n",
    "            sentiment_current = 'positive'\n",
    "        elif str(j).find('Negative')!= -1:\n",
    "            sentiment_current = 'negative'\n",
    "        elif str(j).find('Mention')== -1 and str(j).find('Positive')== -1 and str(j).find('Negative')== -1:\n",
    "            save_mention(j.name.lower(),j.aspect,'',subclass_current,j.lex)\n",
    "###### Tambahan\n",
    "        if str(j).find('Ambience')!= -1:         \n",
    "            aspect_current = ['AMBIENCE#GENERAL']\n",
    "        elif str(j).find('Price')!= -1:\n",
    "            aspect_current = ['DRINKS#PRICES', 'FOOD#PRICES']\n",
    "        elif str(j).find('Service')!= -1:\n",
    "            aspect_current = ['SERVICE#GENERAL']\n",
    "###### Tmbahan            \n",
    "\n",
    "        for k in j.subclasses():\n",
    "            sentiment_current_lv2 = ''\n",
    "            aspect_current_lv2 = []\n",
    "            aspect_current_lv2 = k.aspect\n",
    "            if str(k).find('Positive')!= -1:\n",
    "                sentiment_current_lv2 = 'positive'\n",
    "            elif str(k).find('Negative')!= -1:\n",
    "                sentiment_current_lv2 = 'negative'\n",
    "            if str(k).find('Mention')== -1 and str(k).find('Positive')== -1 and str(k).find('Negative')== -1:\n",
    "                save_mention(k.name.lower(),aspect_current,sentiment_current,subclass_current,k.lex)\n",
    "            for l in k.subclasses():\n",
    "                if sentiment_current_lv2!='' and aspect_current_lv2:\n",
    "                    save_mention(l.name.lower(),aspect_current_lv2,sentiment_current_lv2,subclass_current,l.lex)\n",
    "                elif sentiment_current_lv2!='':\n",
    "                    save_mention(l.name.lower(),aspect_current,sentiment_current_lv2,subclass_current,l.lex)\n",
    "                elif aspect_current_lv2:\n",
    "                    save_mention(l.name.lower(),aspect_current_lv2,sentiment_current,subclass_current,l.lex)\n",
    "                else:\n",
    "                    save_mention(l.name.lower(),aspect_current,sentiment_current,subclass_current,l.lex)\n",
    "                for m in l.subclasses():\n",
    "                    if sentiment_current_lv2!='' and aspect_current_lv2:\n",
    "                        save_mention(m.name.lower(),aspect_current_lv2,sentiment_current_lv2,subclass_current,m.lex)\n",
    "                    elif sentiment_current_lv2!='':\n",
    "                        save_mention(m.name.lower(),aspect_current,sentiment_current_lv2,subclass_current,m.lex)\n",
    "                    elif aspect_current_lv2:\n",
    "                        save_mention(m.name.lower(),aspect_current_lv2,sentiment_current,subclass_current,m.lex)\n",
    "                    else:\n",
    "                        save_mention(m.name.lower(),aspect_current,sentiment_current,subclass_current,m.lex)\n",
    "                    for n in m.subclasses():\n",
    "                        if sentiment_current_lv2!='' and aspect_current_lv2:\n",
    "                            save_mention(n.name.lower(),aspect_current_lv2,sentiment_current_lv2,subclass_current,n.lex)\n",
    "                        elif sentiment_current_lv2!='':\n",
    "                            save_mention(n.name.lower(),aspect_current,sentiment_current_lv2,subclass_current,n.lex)\n",
    "                        elif aspect_current_lv2:\n",
    "                            save_mention(n.name.lower(),aspect_current_lv2,sentiment_current,subclass_current,n.lex)\n",
    "                        else:\n",
    "                            save_mention(n.name.lower(),aspect_current,sentiment_current,subclass_current,n.lex)\n",
    "\n",
    "sentiment_term = []\n",
    "sentiment = []\n",
    "sentiment_aspect = []\n",
    "for i in onto.Sentiment.subclasses():\n",
    "#     print('*'+str(i))\n",
    "    sentiment_current = ''\n",
    "    sentiment_current = str(i.name.lower())\n",
    "    for j in i.subclasses():\n",
    "        aspect_current = []\n",
    "        ###### Tambahan\n",
    "        if str(j).find('Ambience')!= -1:         \n",
    "            aspect_current = ['AMBIENCE#GENERAL']\n",
    "        elif str(j).find('Price')!= -1:\n",
    "            aspect_current = ['DRINKS#PRICES', 'FOOD#PRICES']\n",
    "        elif str(j).find('Service')!= -1:\n",
    "            aspect_current = ['SERVICE#GENERAL']\n",
    "        ###### Tmbahan\n",
    "        for k in j.subclasses():\n",
    "            sentiment_term.append(k.name.lower())\n",
    "            sentiment.append(sentiment_current)\n",
    "            sentiment_aspect.append(aspect_current)\n",
    "\n",
    "mention.append(\"beautiful\")\n",
    "mention_aspect.append([])\n",
    "mention_sentiment.append(\"positive\")\n",
    "mention_subclass.append(\"\")\n",
    "mention_lex.append([\"beautiful\"])\n",
    "\n",
    "print(\"{:<14} {:<13} {:<15} {:<10}\".format(\"Term\", \"Sentiment\",\"Aspect\", \"Sub_Class\"))\n",
    "print(\"---------------------------------------------------------------\")\n",
    "\n",
    "for i in range(len(mention)):\n",
    "# for i in range(10):\n",
    "    # if len(mention_aspect[i])==1:\n",
    "    #   print (\"{:<14} {:<13} {:<15} {:<10} {:<15}\".format(str(mention[i]), str(mention_sentiment[i]),str(mention_aspect[i]), str(mention_subclass[i]),str(mention_lex[i])))\n",
    "    if mention[i] =='expensive':\n",
    "      print (\"{:<14} {:<13} {:<15} {:<10} {:<15}\".format(str(mention[i]), str(mention_sentiment[i]),str(mention_aspect[i]), str(mention_subclass[i]),str(mention_lex[i])))\n",
    "print(\"\")  \n",
    "# print (\"{:<14} {:<13} {:<15} \".format(\"Term\", \"Sentiment\",\"Aspect\"))\n",
    "# print(\"---------------------------------------------------------------\")\n",
    "# for i in range(len(sentiment_term)):\n",
    "#     print (\"{:<14} {:<13} {:<15} \".format(str(sentiment_term[i]), str(sentiment[i]),str(sentiment_aspect[i])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sinonim Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/alishalistya/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bench', 'patio', 'terrace', 'terrasse'}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def sinonim(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lm in syn.lemmas():\n",
    "                 synonyms.append(lm.name())#adding into synonyms\n",
    "    return(set(synonyms))\n",
    "sinonim(\"enjoy\")\n",
    "sinonim(\"terrace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decrease', 'decrement'}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def antonim(word):\n",
    "    antonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lm in syn.lemmas():\n",
    "            if lm.antonyms():\n",
    "                antonyms.append(lm.antonyms()[0].name()) #adding into antonyms\n",
    "    return(set(antonyms))\n",
    "antonim(\"increase\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fitur sentiment dan aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiture_sentiment = []\n",
    "fiture_aspect = []\n",
    "for i in range(len(mention)):\n",
    "    if mention[i] not in fiture_aspect:\n",
    "      fiture_aspect.append(mention[i])\n",
    "    if mention[i] not in fiture_sentiment:\n",
    "      fiture_sentiment.append(mention[i])\n",
    "    if mention_sentiment[i] != '':\n",
    "        for sin in sinonim(mention[i]): # untuk memasukan ekstraksi fiture sinonim\n",
    "            if sin not in fiture_sentiment:\n",
    "              fiture_sentiment.append(sin)\n",
    "\n",
    "    if len(mention_aspect[i])!=0:\n",
    "        for sin in sinonim(mention[i]): # untuk memasukan ekstraksi fiture sinonim\n",
    "            if sin not in fiture_aspect:\n",
    "              fiture_aspect.append(sin)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jalankan Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['judging', 'from', 'previous', 'posts', 'this', 'used', 'to', 'be', 'a', 'good', 'place', ',', 'but', 'not', 'any', 'longer', '.']\n",
      "['we', ',', 'there', 'were', 'four', 'of', 'us', ',', 'arrived', 'at', 'noon', '-', 'the', 'place', 'was', 'empty', '-', 'and', 'the', 'staff', 'acted', 'like', 'we', 'were', 'imposing', 'on', 'them', 'and', 'they', 'were', 'very', 'rude', '.']\n",
      "['they', 'never', 'brought', 'us', 'complimentary', 'noodles', ',', 'ignored', 'repeated', 'requests', 'for', 'sugar', ',', 'and', 'threw', 'our', 'dishes', 'on', 'the', 'table', '.']\n",
      "['the', 'food', 'was', 'lousy', '-', 'too', 'sweet', 'or', 'too', 'salty']\n",
      "['the', 'portions', 'tiny', '.']\n",
      "['the', 'food', 'was', 'well', 'prepared']\n"
     ]
    }
   ],
   "source": [
    "for x in range(len(xml_text)):\n",
    "    xml_text[x] = preprocessing(xml_text[x])\n",
    "    \n",
    "for x in range(5):\n",
    "    print(xml_text[x])\n",
    "print(\"['the', 'food', 'was', 'well', 'prepared']\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import svm, datasets\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "temp_arr = []\n",
    "\n",
    "\n",
    "X = xml_text\n",
    "y = xml_sentiment\n",
    "y_aspect = xml_aspect\n",
    "\n",
    "text_train, text_test, sentiment_train, sentiment_test = model_selection.train_test_split(X, y, train_size=0.70, test_size=0.30, random_state=101)\n",
    "text_train, text_test, aspect_train, aspect_test = model_selection.train_test_split(X, y_aspect, train_size=0.70, test_size=0.30, random_state=101)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klasifikasi Ontology Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "618\n",
      "0.0\n",
      "307\n",
      "[1, 2, 3, 7, 8, 10, 11, 14, 16, 28, 30, 32, 34, 39, 40, 41, 42, 44, 48, 49, 50, 51, 53, 54, 56, 57, 61, 62, 65, 67, 70, 71, 77, 78, 79, 84, 86, 90, 91, 93, 94, 95, 99, 100, 102, 103, 104, 105, 106, 107, 108, 109, 110, 113, 115, 117, 120, 122, 123, 124, 125, 126, 130, 131, 132, 134, 138, 139, 140, 141, 142, 143, 144, 145, 147, 153, 154, 156, 160, 161, 163, 167, 168, 169, 171, 172, 173, 174, 175, 176, 177, 179, 181, 183, 186, 191, 192, 193, 201, 202, 203, 204, 207, 209, 210, 211, 212, 213, 215, 217, 219, 220, 222, 230, 231, 232, 233, 235, 236, 237, 238, 240, 241, 244, 247, 249, 253, 255, 256, 257, 258, 260, 262, 264, 265, 267, 268, 270, 273, 274, 276, 277, 278, 279, 281, 283, 284, 287, 288, 290, 292, 293, 294, 295, 299, 300, 302, 304, 305, 307, 314, 319, 321, 324, 325, 328, 331, 332, 333, 338, 339, 345, 347, 348, 349, 352, 355, 357, 360, 361, 362, 363, 364, 368, 372, 373, 374, 375, 376, 381, 387, 389, 390, 391, 396, 397, 398, 400, 402, 403, 404, 408, 411, 414, 416, 420, 421, 422, 428, 430, 431, 436, 438, 440, 442, 444, 445, 447, 450, 453, 456, 458, 460, 461, 463, 465, 468, 470, 471, 472, 478, 479, 482, 484, 487, 488, 489, 490, 491, 493, 494, 500, 502, 505, 506, 509, 512, 514, 515, 517, 519, 520, 524, 525, 526, 527, 528, 530, 531, 533, 534, 535, 536, 539, 540, 542, 543, 547, 550, 552, 553, 555, 556, 557, 558, 560, 562, 563, 564, 565, 566, 567, 575, 578, 580, 581, 583, 584, 585, 586, 587, 590, 591, 594, 596, 599, 600, 602, 604, 605, 607, 609, 610, 613, 614, 616, 617]\n",
      "['negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive']\n",
      "[2, 10, 12, 14, 15, 16, 21, 24, 26, 28, 47, 49, 54, 56, 61, 63, 64, 66, 67, 69, 70, 71, 73, 75, 77, 81, 85, 86, 93, 94, 99, 102, 105, 107, 112, 116, 124, 131, 132, 133, 136, 137, 140, 143, 144, 153, 162, 163, 164, 171, 175, 178, 181, 183, 185, 186, 188, 193, 202, 209, 210, 217, 218, 222, 225, 230, 231, 232, 237, 238, 240, 241, 244, 245, 247, 249, 250, 255, 259, 265, 269, 273, 276, 279, 281, 289, 290, 292, 296, 300, 301, 310, 313, 314, 326, 331, 338, 339, 340, 344, 348, 351, 354, 355, 356, 360, 364, 368, 370, 373, 388, 390, 391, 393, 406, 407, 409, 416, 421, 425, 429, 434, 435, 438, 444, 447, 450, 454, 456, 457, 461, 462, 463, 464, 465, 466, 473, 477, 482, 483, 484, 486, 487, 489, 490, 497, 500, 501, 506, 508, 509, 512, 523, 524, 530, 531, 535, 538, 540, 541, 542, 555, 556, 559, 562, 571, 576, 586, 587, 589, 595, 599, 606, 611, 612, 614]\n",
      "[['RESTAURANT#GENERAL'], ['RESTAURANT#MISCELLANEOUS'], ['SERVICE#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['LOCATION#GENERAL'], ['LOCATION#GENERAL'], ['SERVICE#GENERAL'], ['SERVICE#GENERAL'], ['RESTAURANT#GENERAL'], ['SERVICE#GENERAL'], ['AMBIENCE#GENERAL'], ['SERVICE#GENERAL'], ['LOCATION#GENERAL'], ['LOCATION#GENERAL'], ['RESTAURANT#MISCELLANEOUS'], ['SERVICE#GENERAL'], ['RESTAURANT#GENERAL'], ['LOCATION#GENERAL'], ['LOCATION#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#MISCELLANEOUS'], ['LOCATION#GENERAL'], ['LOCATION#GENERAL'], ['RESTAURANT#MISCELLANEOUS'], ['AMBIENCE#GENERAL'], ['RESTAURANT#MISCELLANEOUS'], ['RESTAURANT#MISCELLANEOUS'], ['SERVICE#GENERAL'], ['LOCATION#GENERAL'], ['RESTAURANT#MISCELLANEOUS'], ['LOCATION#GENERAL'], ['RESTAURANT#GENERAL'], ['LOCATION#GENERAL'], ['AMBIENCE#GENERAL'], ['RESTAURANT#MISCELLANEOUS'], ['SERVICE#GENERAL'], ['RESTAURANT#GENERAL'], ['LOCATION#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['SERVICE#GENERAL'], ['LOCATION#GENERAL'], ['RESTAURANT#GENERAL'], ['AMBIENCE#GENERAL'], ['RESTAURANT#MISCELLANEOUS'], ['RESTAURANT#GENERAL'], ['AMBIENCE#GENERAL'], ['AMBIENCE#GENERAL'], ['RESTAURANT#GENERAL'], ['LOCATION#GENERAL'], ['LOCATION#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#MISCELLANEOUS'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['AMBIENCE#GENERAL'], ['LOCATION#GENERAL'], ['SERVICE#GENERAL'], ['RESTAURANT#MISCELLANEOUS'], ['SERVICE#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#MISCELLANEOUS'], ['AMBIENCE#GENERAL'], ['SERVICE#GENERAL'], ['RESTAURANT#GENERAL'], ['AMBIENCE#GENERAL'], ['AMBIENCE#GENERAL'], ['RESTAURANT#GENERAL'], ['AMBIENCE#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['SERVICE#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['AMBIENCE#GENERAL'], ['RESTAURANT#GENERAL'], ['LOCATION#GENERAL'], ['RESTAURANT#GENERAL'], ['SERVICE#GENERAL'], ['RESTAURANT#MISCELLANEOUS'], ['SERVICE#GENERAL'], ['SERVICE#GENERAL'], ['AMBIENCE#GENERAL'], ['AMBIENCE#GENERAL'], ['AMBIENCE#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#MISCELLANEOUS'], ['RESTAURANT#MISCELLANEOUS'], ['AMBIENCE#GENERAL'], ['SERVICE#GENERAL'], ['RESTAURANT#GENERAL'], ['SERVICE#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['AMBIENCE#GENERAL'], ['RESTAURANT#GENERAL'], ['AMBIENCE#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['AMBIENCE#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#MISCELLANEOUS'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#MISCELLANEOUS'], ['SERVICE#GENERAL'], ['AMBIENCE#GENERAL'], ['RESTAURANT#MISCELLANEOUS'], ['RESTAURANT#MISCELLANEOUS'], ['RESTAURANT#GENERAL'], ['LOCATION#GENERAL'], ['AMBIENCE#GENERAL'], ['SERVICE#GENERAL'], ['AMBIENCE#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['SERVICE#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#MISCELLANEOUS'], ['AMBIENCE#GENERAL'], ['SERVICE#GENERAL'], ['LOCATION#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['AMBIENCE#GENERAL'], ['SERVICE#GENERAL'], ['RESTAURANT#GENERAL'], ['SERVICE#GENERAL'], ['RESTAURANT#MISCELLANEOUS'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['LOCATION#GENERAL'], ['LOCATION#GENERAL'], ['RESTAURANT#GENERAL'], ['AMBIENCE#GENERAL'], ['SERVICE#GENERAL'], ['AMBIENCE#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['AMBIENCE#GENERAL'], ['SERVICE#GENERAL'], ['SERVICE#GENERAL'], ['RESTAURANT#GENERAL'], ['AMBIENCE#GENERAL'], ['SERVICE#GENERAL'], ['RESTAURANT#MISCELLANEOUS'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['AMBIENCE#GENERAL'], ['SERVICE#GENERAL'], ['LOCATION#GENERAL'], ['AMBIENCE#GENERAL'], ['AMBIENCE#GENERAL'], ['AMBIENCE#GENERAL'], ['RESTAURANT#GENERAL'], ['AMBIENCE#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL'], ['LOCATION#GENERAL'], ['AMBIENCE#GENERAL'], ['AMBIENCE#GENERAL'], ['AMBIENCE#GENERAL'], ['RESTAURANT#GENERAL'], ['RESTAURANT#GENERAL']]\n"
     ]
    }
   ],
   "source": [
    "print(len(text_test))\n",
    "from strsimpy.qgram import QGram\n",
    "\n",
    "qgram = QGram(2)\n",
    "print(qgram.distance('Doni-aaaa', 'Doni-aaaa'))\n",
    "\n",
    "me = MongeElkan()\n",
    "arr_id_klasi_onto = []\n",
    "arr_senti_klasi_onto = []\n",
    "\n",
    "arr_id_aspect_klasi_onto = []\n",
    "arr_aspect_klasi_onto = []\n",
    "\n",
    "xx = 0\n",
    "for z in range(len(text_test)):\n",
    "  temp = 10\n",
    "  temp2 = 10\n",
    "  conflict = 0 # 0 no conflict - 1 conflict\n",
    "  jml_match = 0\n",
    "  jml_match_aspect = 0\n",
    "  conflict_aspect = 0 # 0 no conflict - 1 conflict\n",
    "  for zz in range(len(text_test[z])):\n",
    "    for i in range(len(mention)):\n",
    "       if (mention_sentiment[i] != ''):\n",
    "        score = qgram.distance(str([mention[i]]), str([text_test[z][zz]]))\n",
    "        if score < 1 :\n",
    "          jml_match = jml_match + 1\n",
    "          if jml_match == 1:\n",
    "            hasil_sentiment_klasi_onto = mention_sentiment[i]\n",
    "            hasil_id_klasi_onto = z\n",
    "          elif hasil_sentiment_klasi_onto != mention_sentiment[i]:\n",
    "            conflict = 1\n",
    "    for i in range(len(mention)):\n",
    "       if len(mention_aspect[i])==1:\n",
    "        score2 = qgram.distance(str([mention[i]]), str([text_test[z][zz]]))\n",
    "        if score2 < 1 :\n",
    "          jml_match_aspect = jml_match_aspect + 1\n",
    "          if jml_match_aspect == 1:\n",
    "            hasil_aspect_klasi_onto = mention_aspect[i]\n",
    "            hasil_id_aspect_klasi_onto = z\n",
    "          elif hasil_aspect_klasi_onto != mention_aspect[i]:\n",
    "            conflict_aspect = 1\n",
    "  if jml_match > 0 and conflict != 1:\n",
    "    arr_id_klasi_onto.append(hasil_id_klasi_onto)\n",
    "    arr_senti_klasi_onto.append(hasil_sentiment_klasi_onto)\n",
    "  if jml_match_aspect > 0 and conflict_aspect != 1:\n",
    "    arr_id_aspect_klasi_onto.append(hasil_id_aspect_klasi_onto)\n",
    "    arr_aspect_klasi_onto.append(hasil_aspect_klasi_onto)\n",
    "  xx = xx + 1\n",
    "\n",
    "print(len(arr_id_klasi_onto))\n",
    "print(arr_id_klasi_onto)\n",
    "print(arr_senti_klasi_onto)\n",
    "\n",
    "print(arr_id_aspect_klasi_onto)\n",
    "print(arr_aspect_klasi_onto)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-['beautiful'] ['fish']\n",
      "negativepositive\n",
      "negative617\n"
     ]
    }
   ],
   "source": [
    "question_1 = \"you sure get a lot of food for your money.\"\n",
    "question_1  = preprocessing(question_1 )\n",
    "\n",
    "for word in range(len(question_1 )):\n",
    "  for i in range(len(mention)):\n",
    "       if (mention_sentiment[i] != ''):\n",
    "        score = qgram.distance(str([mention[i]]), str([question_1 [word]]))\n",
    "        if score < 1 :\n",
    "          # print(qgram.distance(str([mention[i]]), str([text_test[z][zz]])))\n",
    "          # print(text_test[z][zz]+str(mention[i])+\" \"+str(text_test[z][zz]))\n",
    "          jml_match = jml_match + 1\n",
    "          if jml_match == 1:\n",
    "            # temp = score\n",
    "            hasil_sentiment_klasi_onto = mention_sentiment[i]\n",
    "            hasil_id_klasi_onto = z\n",
    "            # print(\"1-\"+str([mention[i]]), str([text_test[z][zz]]))\n",
    "          elif hasil_sentiment_klasi_onto != mention_sentiment[i]:\n",
    "            hasil_sentiment_klasi_onto = mention_sentiment[i]\n",
    "            hasil_id_klasi_onto = z\n",
    "            # print(\"2-\"+str([mention[i]]), str([text_test[z][zz]]))\n",
    "            # print(hasil_sentiment_klasi_onto + mention_sentiment[i])\n",
    "            conflict = 1\n",
    "# print(\"3-\"+str([mention[i]]), str([text_test[z][zz]]))\n",
    "# print(hasil_sentiment_klasi_onto + mention_sentiment[i])\n",
    "print(str(hasil_sentiment_klasi_onto)+str(hasil_id_klasi_onto))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
